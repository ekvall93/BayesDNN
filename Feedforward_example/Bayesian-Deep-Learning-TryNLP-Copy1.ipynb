{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.distributions import Bernoulli\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "from libKMCUDA import kmeans_cuda\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#model = gensim.models.Word2Vec.load(\"assets/doc2vecModels/KTH2004_i10000_w10_d500_plainTrain/KTH2004_i10000_w10_d500_plainTrain.model\")\n",
    "#model = gensim.models.Word2Vec.load(\"assets/doc2vecModels/KTH2004_i2000_w10_d500_plainTrain_small/KTH2005_i2000_w10_d500_plainTrain\")\n",
    "model = gensim.models.Word2Vec.load(\"../../assets/doc2vecModels/KTH2004_i2000_w10_d500_plainTrain_small_noAuth/KTH2005_i2000_w10_d500_plainTrain_noAuth\")\n",
    "doc_vec = model.docvecs.vectors_docs\n",
    "doc_tag = list(model.docvecs.doctags.keys())\n",
    "word_vectors = model.wv.vectors\n",
    "\n",
    "pickle_o = pickle_obj(); \n",
    "id_to_auth = pickle_o.load(\"../../assets/dictionaries/id_to_all_auths_2004\")\n",
    "auth_to_id = pickle_o.load(\"../../assets/dictionaries/auths_to_all_id_2004\")\n",
    "id_to_auth = pickle_o.load(\"../../assets/dictionaries/id_to_all_auths_2004\")\n",
    "auth_to_id = pickle_o.load(\"../../assets/dictionaries/auths_to_all_id_2004\")\n",
    "\n",
    "negative_test = pickle_o.load(\"../../assets/goldenstandards/test_neg_aricles\")\n",
    "negative_pos = pickle_o.load(\"../../assets/goldenstandards/test_pos_aricles\")\n",
    "\n",
    "df_auth = pd.read_csv(\"../../assets/dataframes/KT_auth_2004\")\n",
    "df_abs = pd.read_csv(\"../../assets/dataframes/all_authors_df_2004\")\n",
    "\n",
    "#negative_test = pickle_o.load(\"assets/goldenstandards/test_neg_aricles\")\n",
    "#negative_pos = pickle_o.load(\"assets/goldenstandards/test_pos_aricles\")\n",
    "\n",
    "negative_val = np.load(\"../../assets/goldenstandards/NLS_val.npy\")\n",
    "negative_test = np.load(\"../../assets/goldenstandards/NLS_test.npy\")\n",
    "postive_val = np.load(\"../../assets/goldenstandards/LS_val.npy\")\n",
    "positve_test = np.load(\"../../assets/goldenstandards/LS_test.npy\")\n",
    "\n",
    "\n",
    "tes_tag_str = list(np.array(list(negative_val) + list(negative_test)+list(postive_val)+list(positve_test)).astype(str))\n",
    "train_tag = list(set(doc_tag) - (set(tes_tag_str)))\n",
    "train_vec = model[train_tag]\n",
    "\n",
    "test_nls = list(np.asarray(negative_test).astype(str))\n",
    "val_nls = list(np.asarray(negative_val).astype(str))\n",
    "test_ls = list(np.asarray(positve_test).astype(str))\n",
    "val_ls = list(np.asarray(postive_val).astype(str))\n",
    "\n",
    "X_nls_test = model[test_nls]\n",
    "X_nls_val = model[val_nls]\n",
    "\n",
    "X_ls_test = model[test_ls]\n",
    "X_ls_val = model[val_ls]\n",
    "\n",
    "\n",
    "y_nls_test = np.ones(X_nls_test.shape[0])\n",
    "y_nls_val = np.ones(X_nls_val.shape[0])\n",
    "\n",
    "y_ls_test = np.zeros(X_ls_test.shape[0])\n",
    "y_ls_val = np.zeros(X_ls_val.shape[0])\n",
    "\n",
    "\n",
    "X_test = np.concatenate((X_nls_test, X_ls_test))\n",
    "X_val = np.concatenate((X_nls_val, X_ls_val))\n",
    "\n",
    "y_test = np.concatenate((y_nls_test, y_ls_test))\n",
    "y_val = np.concatenate((y_nls_val, y_ls_val))\n",
    "\n",
    "all_vec = np.concatenate([train_vec, word_vectors])\n",
    "tags =np.concatenate([train_tag, model.wv.index2word])\n",
    "all_vec = np.asarray(all_vec)\n",
    "tags =np.asarray(tags)\n",
    "\n",
    "col_names = pd.read_csv('../../assets/clusterModels/assignKMM_clusters1000Articles', nrows=0).columns\n",
    "types_dict = {}\n",
    "types_dict.update({col: str for col in col_names if col not in types_dict})\n",
    "df = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters1000Articles\", dtype=types_dict)\n",
    "df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "col_names = pd.read_csv('../../assets/clusterModels/assignKMM_clusters1000Articles_clusters_with_only_authors', nrows=0).columns\n",
    "types_dict = {}\n",
    "types_dict.update({col: str for col in col_names if col not in types_dict})\n",
    "df = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters1000Articles_clusters_with_only_authors\", dtype=types_dict)\n",
    "df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "\n",
    "col_names = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters300Articles_df_cluster_authors\", nrows=0).columns\n",
    "types_dict = {}\n",
    "types_dict.update({col: str for col in col_names if col not in types_dict})\n",
    "\n",
    "df_cluster_authors = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters300Articles_df_cluster_authors\", dtype=types_dict)\n",
    "df_cluster_authors.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "df_cluster_authors.columns = df_cluster_authors.columns.astype(int)\n",
    "\n",
    "postive_clusters = pickle_o.load(\"../../assets/clusterModels/KMM1000_testVal_postive_clusters\")\n",
    "negaitves_clusters = pickle_o.load(\"../../assets/clusterModels/KMM1000_testVal_negaitves_clusters\")\n",
    "\n",
    "p_list_cluster = list(set([item[0] for sublist in postive_clusters for item in sublist]))\n",
    "n_list_cluster = list(set([item[0] for sublist in negaitves_clusters for item in sublist]))\n",
    "intersect = list(set(n_list_cluster) & set(p_list_cluster))\n",
    "n_list_cluster = list(set(n_list_cluster) ^ set(intersect))\n",
    "\n",
    "not_NLS = [667, 759, 118, 237, 779, 815, 166, 716, 832, 771, 400, 200, 515, 691, 357, 19, 29, 333, 227, 277]\n",
    "extra_ls = [759, 118, 237, 779, 716, 832, 771, 400, 200, 691, 357, 333, 227, 277]\n",
    "LS = [115, 329, 423, 680, 562, 45, 178, 624, 143, 602, 784, 748, 788, 763, 645, 328, 564, 919, 444, 892, 962\n",
    "     , 24, 632, 160, 187, 936, 972, 76, 163, 533, 841, 230, 416]\n",
    "NLS = list((set(n_list_cluster) - set(not_NLS)))\n",
    "#LS = LS + extra_ls\n",
    "potential_ls = list((set(df.columns.values.astype(int)) - set(LS)) - set(NLS))\n",
    "\n",
    "#NLF_train_v, nlf_train_lab = get_labels_and_vec(df, NLS, 1)\n",
    "#LF_train_v, lf_train_lab = get_labels_and_vec(df, LS, 1)\n",
    "\n",
    "NLF_train_v, nlf_train_lab = get_train_data(df, NLS, model, 1)\n",
    "LF_train_v, lf_train_lab = get_train_data(df, LS, model, 1)\n",
    "\n",
    "X_train, y_train = concatenate_and_get_labels(LF_train_v, NLF_train_v)\n",
    "X_un_pot, L_pot = getVectors(df, model, potential_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c \"import tensorflow as tf; print(tf.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLayer:\n",
    "    \"\"\"\n",
    "    StochasticLayer Dense Layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    BN : bool\n",
    "        Batchnormalization\n",
    "    n_in : int\n",
    "        Input nodes\n",
    "    n_out: int\n",
    "        Output nodes\n",
    "    model_prob: float\n",
    "        Dropout probability\n",
    "    model_lam: float\n",
    "        regualarization term\n",
    "    is_training: bool\n",
    "        traing-phase/test-phase        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Outputlayer\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, n_in, n_out, model_prob, model_lam, is_training, BN=False, decay=0.99):\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.is_training = is_training    \n",
    "        self.model_prob = model_prob\n",
    "        self.model_lam = model_lam\n",
    "        self.model_bern = Bernoulli(probs=self.model_prob, dtype=tf.float32)\n",
    "        self.model_M = tf.Variable(tf.truncated_normal([n_in, n_out], stddev=0.01))\n",
    "        self.model_m = tf.Variable(tf.zeros([n_out]))\n",
    "        self.model_W = tf.matmul(\n",
    "            tf.diag(self.model_bern.sample((n_in, ))), self.model_M\n",
    "        )\n",
    "        #Parameters for BatchNormalization\n",
    "        self.BN = BN\n",
    "        self.scale = tf.Variable(tf.ones([n_out]))\n",
    "        self.beta = tf.Variable(tf.zeros([n_out]))\n",
    "        self.epsilon = 1e-2\n",
    "        self.mean = tf.Variable(tf.zeros([n_out]), trainable=False)\n",
    "        self.var = tf.Variable(tf.ones([n_out]), trainable=False)\n",
    "        self.decay = decay\n",
    "\n",
    "    def __call__(self, X, activation=tf.identity):\n",
    "        output = activation(tf.matmul(X, self.model_W) + self.model_m)\n",
    "        \n",
    "        if self.BN:\n",
    "            output = self.batch_norm_wrapper(output, decay = 0.5)\n",
    "                \n",
    "        if self.model_M.shape[1] == 1:\n",
    "            #output = tf.squeeze(output)\n",
    "            output = output\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def regularization(self):\n",
    "        \"\"\"regularization\"\"\"\n",
    "\n",
    "        return self.model_lam * (\n",
    "            self.model_prob * tf.reduce_sum(tf.square(self.model_M)) +\n",
    "            tf.reduce_sum(tf.square(self.model_m))\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    def batch_norm_wrapper(self, inputs, decay = 0.99):\n",
    "        \"\"\"\n",
    "        Batchnormalization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : array\n",
    "            Batch input\n",
    "        is_training : bool\n",
    "            training phase/test phase\n",
    "        decay: float\n",
    "            Decrease training of popoulation mean and variance.    \n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            Normalized batched\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        def BN_train():\n",
    "            \"\"\"\n",
    "            Batchnormalization training\n",
    " \n",
    "            Returns\n",
    "            -------\n",
    "            array\n",
    "                Normalized train batch\n",
    "            \"\"\"\n",
    "            batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "            train_mean = tf.assign(self.mean, self.mean * self.decay + batch_mean * (1 - self.decay))\n",
    "            train_var = tf.assign(self.var, self.var * self.decay + batch_var * (1 - self.decay))\n",
    "            #train_mean = tf.assign(self.mean, self.mean + batch_mean)\n",
    "            #train_var = tf.assign(self.var, self.var + batch_var)\n",
    "            \n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                return tf.nn.batch_normalization(inputs,\n",
    "                    batch_mean, batch_var, self.beta, self.scale, self.epsilon)\n",
    "            \n",
    "        def BN_test():\n",
    "            \"\"\"\n",
    "            Batchnormalization test\n",
    " \n",
    "            Returns\n",
    "            -------\n",
    "            array\n",
    "                Normalized test batched\n",
    "            \"\"\"\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                self.mean, self.var, self.beta, self.scale, self.epsilon)\n",
    "        \n",
    "        \n",
    "        return tf.cond(self.is_training, BN_train, BN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, glob\n",
    " \n",
    "def moveAllFilesinDir(srcDir, dstDir):\n",
    "    # Check if both the are directories\n",
    "    if os.path.isdir(srcDir) and os.path.isdir(dstDir) :\n",
    "        # Iterate over all the files in source directory\n",
    "        for filePath in glob.glob(srcDir + '\\*'):\n",
    "            print(filePath)\n",
    "            # Move each file to destination Directory\n",
    "            shutil.move(filePath, dstDir);\n",
    "    else:\n",
    "        print(\"srcDir & dstDir should be Directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDNNCalssifier:\n",
    "    def __init__(self, batchnorm=True, dropout_fit=0.8, dropout_sample=0.8, batch_size=256, mc_samples=800):\n",
    "        self.BN = batchnorm\n",
    "        self.dropout_fit = dropout_fit\n",
    "        self.dropout_sample = dropout_sample\n",
    "        self.bs = batch_size\n",
    "        self.load = False\n",
    "        self.mc_samples = mc_samples\n",
    "    def _build_graph(self, n_feats, n_samples, dp_prob=1.0, BN=False, decay=0.99):\n",
    "        \"\"\"\n",
    "        Building training graph\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dp_prob : float\n",
    "            Dropout prob\n",
    "        BN : bool\n",
    "            Batchnorm\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            graph outputs\n",
    "        \"\"\"\n",
    "    \n",
    "        #n_feats = X.shape[1]\n",
    "        n_hidden = 500\n",
    "        model_prob = dp_prob\n",
    "        model_lam = 1e-8\n",
    "        BN=BN\n",
    "    \n",
    "        X_input = tf.placeholder(tf.float32, [None, n_feats])\n",
    "        y_input = tf.placeholder(tf.float32, [None,1])\n",
    "    \n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "        Layer_1 = StochasticLayer(n_feats, n_hidden * 2, model_prob, model_lam, is_training ,BN=BN, decay=decay)\n",
    "        Layer_2 = StochasticLayer(n_hidden * 2, n_hidden, model_prob, model_lam, is_training,BN=BN, decay=decay)\n",
    "        Layer_3 = StochasticLayer(n_hidden, n_hidden // 2, model_prob, model_lam, is_training,BN=BN, decay=decay)\n",
    "        Layer_4 = StochasticLayer(n_hidden // 2, 1, model_prob, model_lam, is_training, BN=BN, decay=decay)\n",
    "    \n",
    "        z_1 = Layer_1(X_input, tf.nn.sigmoid)\n",
    "        z_2 = Layer_2(z_1, tf.nn.sigmoid)\n",
    "        z_3 = Layer_3(z_2, tf.nn.sigmoid)\n",
    "        y_pred = Layer_4(z_3, tf.nn.sigmoid)\n",
    "    \n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_input, logits=y_pred)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "        predicted = tf.nn.sigmoid(y_pred)\n",
    "\n",
    "        correct_pred = tf.equal(tf.round(predicted), y_input)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "        model_loss = (\n",
    "            # Negative log-likelihood.\n",
    "            loss +\n",
    "            # Regularization.\n",
    "            Layer_1.regularization +\n",
    "            Layer_2.regularization +\n",
    "            Layer_3.regularization\n",
    "                    ) / n_samples\n",
    "    \n",
    "        train_step = tf.train.AdamOptimizer(1e-3).minimize(model_loss)\n",
    "\n",
    "        \n",
    "        return (X_input, y_input), train_step, accuracy, model_loss, predicted, tf.train.Saver(), is_training\n",
    "\n",
    "    \n",
    "    def _train(self, X, y, train_step, X_input, y_input, is_training, accuracy, model_mse, saver, \n",
    "               epoch=20, verbose =True, bs=10):\n",
    "        \"\"\"\n",
    "        Train model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        iterations : int\n",
    "            Number of iterations\n",
    "        verbose : bool\n",
    "            Verbose\n",
    "        bs: int\n",
    "            batch-size\n",
    "        \"\"\"\n",
    "        with tf.Session() as sess:\n",
    "            if self.load:\n",
    "                saver.restore(sess, self.path)\n",
    "                \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epoch):\n",
    "                test_idx = np.arange(0 , len(X))\n",
    "                np.random.shuffle(test_idx)\n",
    "                for i in range(0,len(X), bs): \n",
    "                    X_batch = X[test_idx[i:(i+bs)]]\n",
    "                    y_batch = y[test_idx[i:(i+bs)]]\n",
    "            \n",
    "                    train_step.run(feed_dict={X_input: X_batch, y_input: y_batch, is_training:True})\n",
    "                if verbose:\n",
    "                    print(e)\n",
    "                    mse = sess.run(model_mse, {X_input: X_batch, y_input: y_batch, is_training:True})\n",
    "                    _accuracy = sess.run(accuracy, {X_input: X_batch, y_input: y_batch, is_training:True})\n",
    "                    print(\"Epoch {}, -log(mse): {:.2f} , acc: {:.2f}\".format(e, -np.log(mse), _accuracy))\n",
    "            \n",
    "            self.load = False\n",
    "            saved_model = saver.save(sess, 'temp/temp-bn-save')\n",
    "            sess.close()\n",
    "   \n",
    "    def fit(self,X, y, epoch):\n",
    "        self.X_train = X\n",
    "        tf.reset_default_graph()\n",
    "        (X_input, y_input), train_step, accuracy, model_mse, _, saver, is_training = self._build_graph(X.shape[1],\n",
    "                                                                                                       self.bs,\n",
    "                                                                                                       BN=self.BN, \n",
    "                                                                                                       dp_prob=self.dropout_fit)\n",
    "        self.saver = saver\n",
    "        self._train(X, y.reshape(-1,1), train_step, X_input, y_input, is_training,\n",
    "                    accuracy, model_mse, saver, epoch=epoch, verbose = True, bs=self.bs)\n",
    "    \n",
    "    def save_weights(self):\n",
    "        path = \"/home/ekvall/kth-cluster/kth-cluster/getLifeScience/BayesDNN/Feedforward_example/\"\n",
    "        if os.path.isdir(path + \"bestmodel/temp\"):\n",
    "            shutil.rmtree(path + \"bestmodel/temp\")\n",
    "            \n",
    "        shutil.move(path + \"temp\",\n",
    "                   path + \"bestmodel/\")\n",
    "        \n",
    "    def load_weights(self):\n",
    "        self.load = True\n",
    "        self.path = \"/home/ekvall/kth-cluster/kth-cluster/getLifeScience/BayesDNN/Feedforward_example/bestmodel/temp/temp-bn-save\"\n",
    "\n",
    "        \n",
    "    def _test(self, X_pred, X_input, y_input, train_step, model_pred, saver, is_training,\n",
    "              bs=10,BN=False):\n",
    "        \"\"\"\n",
    "        Test model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : int\n",
    "            Monte Carlo Samples\n",
    "        bs : int\n",
    "            Batchnorm samples\n",
    "        \"\"\"\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            if self.load:\n",
    "                saver.restore(sess, self.path)\n",
    "                print(\"yo\")\n",
    "            else:\n",
    "                saver.restore(sess, 'temp/temp-bn-save')\n",
    "    \n",
    "\n",
    "            Y_sample = np.zeros((self.mc_samples, X_pred.shape[0]))\n",
    "            for i in range(self.mc_samples):       \n",
    "                if BN:\n",
    "                    test_idx = np.arange(0 , len(X_pred))\n",
    "                    np.random.shuffle(test_idx)\n",
    "                    model_pred.eval({X_input: self.X_train[test_idx[:bs]],  is_training: True})\n",
    "                Y_sample[i] = (sess.run(model_pred, {X_input: X_pred, is_training: False})).ravel()\n",
    "            return Y_sample\n",
    "    \n",
    "    def sample_predictions(self, X_pred):\n",
    "        tf.reset_default_graph()\n",
    "        (X_input, y_input), train_step, accuracy, model_mse, model_pred, saver, is_training = self._build_graph(X_train.shape[1],\n",
    "                                                                                                                self.bs,\n",
    "                                                                                                                BN=self.BN, \n",
    "                                                                                                          dp_prob=self.dropout_sample)\n",
    "        Y_sample = self._test(X_pred, X_input, y_input, train_step, model_pred, saver, is_training,\n",
    "                              bs=self.bs, BN=False)\n",
    "        \n",
    "        return Y_sample.round()\n",
    "    \n",
    "    def predict(self, X_pred):\n",
    "        y_samples = self.sample_predictions(X_pred)\n",
    "        pred = y_samples.sum(axis=0)\n",
    "        pred[pred <= self.mc_samples //2] = 0\n",
    "        pred[pred > self.mc_samples //2] = 1\n",
    "        return pred\n",
    "    \n",
    "    def probability(self, X_train, X_pred):\n",
    "        n_1 = self.sample_predictions(X_train, X_pred).sum(axis=0)\n",
    "        N = n_1.copy()\n",
    "        n_0 = self.mc_samples - n_1\n",
    "        N[n_0 >= n_1] = n_0[n_0 >= n_1]\n",
    "        P = N / self.mc_samples * 100\n",
    "        return P\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = BayesDNNCalssifier(batchnorm=True, dropout_fit=0.7, dropout_sample=0.6, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.fit(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = dnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = dnn.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"bestmodel/temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ix = pred == y_val\n",
    "false_ix = np.invert(correct_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pre[false_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[false_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred_pre,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sample.astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename=\"./assets/point_estimate.jpg\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian - Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: \n",
    "\n",
    "Article: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\n",
    "\n",
    "Authors: Yarin Gal, Zoubin Ghahramani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, _, saver, is_training = build_graph(BN=False, dp_prob=0.9)\n",
    "train(verbose = False, bs=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, model_pred, saver, is_training = build_graph(BN=False, dp_prob=0.9)\n",
    "Y_sample_DP = test(mc_samples, bs=n_samples)\n",
    "plot(Y_sample_DP, X_pred, mc_samples,color_line=\"b-\", path=\"./assets/non_DO_BN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian - BatchNormalization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit:\n",
    "\n",
    "Aricle: Bayesian Uncertainty Estimation for Batch Normalized Deep Networks\n",
    "\n",
    "Authors: Mattias Teye, Hossein Azizpour, Kevin Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, _, saver, is_training = build_graph(BN=True, dp_prob=1.0, decay=0.6)\n",
    "train(verbose = False , bs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, model_pred, saver, is_training = build_graph(BN=True, dp_prob=1.0, decay=0.6)\n",
    "Y_sample_BN = test(mc_samples, bs=15,BN=True)\n",
    "plot(Y_sample_BN, X_pred,mc_samples, color_line=\"g-\",path=\"./assets/BN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        for i in range(mc_samples):\n",
    "            plt.plot(X_pred[:, 0], Y_sample_BN[i], \"b-\", alpha=1. / 200)\n",
    "        for i in range(mc_samples):\n",
    "            plt.plot(X_pred[:, 0], Y_sample_DP[i], \"r-\", alpha=1. / 200)\n",
    "        plt.plot(X[:, 0], y, \"g.\")\n",
    "        \n",
    "        \n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        #if path:\n",
    "        #    plt.savefig(path, format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian - BatchNormalization and Drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, _, saver, is_training = build_graph(BN=True, dp_prob=0.85, decay=0.5)\n",
    "train(verbose = False, bs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, model_pred, saver, is_training = build_graph(BN=True, dp_prob=0.85, decay=0.5)\n",
    "Y_sample = test(mc_samples, bs=20,BN=True)\n",
    "plot(Y_sample, X_pred,mc_samples, color_line=\"b-\",path=\"./assets/BN_DO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
