{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.distributions import Bernoulli\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGMM.ipynb\r\n",
      "BGMM.py\r\n",
      "BGMMAuthors-1000-Short.ipynb\r\n",
      "BGMMAuthors-1000-ValTest.ipynb\r\n",
      "BGMMAuthors-1000.ipynb\r\n",
      "BGMMAuthors-2000-Copy1.ipynb\r\n",
      "BGMMAuthors-2000.ipynb\r\n",
      "BGMMAuthors.ipynb\r\n",
      "BayesDNN\r\n",
      "BayesDNNClassifier.py\r\n",
      "README.md\r\n",
      "ScrapeKTH_Positives_Negatives-Real.ipynb\r\n",
      "__pycache__\r\n",
      "assets\r\n",
      "classifyArticles.py\r\n",
      "fraud.ipynb\r\n",
      "kmeans_clustering-1000-Copy1.ipynb\r\n",
      "kmeans_clustering-1000-Test-Val.ipynb\r\n",
      "kmeans_clustering-1000-only-test--95%.ipynb\r\n",
      "kmeans_clustering-500.ipynb\r\n",
      "latent_representation.png\r\n",
      "original.png\r\n",
      "prepareDivaDocuments.ipynb\r\n",
      "semiSupervised.py\r\n",
      "semiSupervisedDnn.py\r\n",
      "semisup\r\n",
      "temp\r\n",
      "tmp\r\n",
      "train2004.py\r\n",
      "train2004_noAuthor.py\r\n",
      "trainDocVec.ipynb\r\n",
      "utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "from libKMCUDA import kmeans_cuda\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#model = gensim.models.Word2Vec.load(\"assets/doc2vecModels/KTH2004_i10000_w10_d500_plainTrain/KTH2004_i10000_w10_d500_plainTrain.model\")\n",
    "#model = gensim.models.Word2Vec.load(\"assets/doc2vecModels/KTH2004_i2000_w10_d500_plainTrain_small/KTH2005_i2000_w10_d500_plainTrain\")\n",
    "model = gensim.models.Word2Vec.load(\"../../assets/doc2vecModels/KTH2004_i2000_w10_d500_plainTrain_small_noAuth/KTH2005_i2000_w10_d500_plainTrain_noAuth\")\n",
    "doc_vec = model.docvecs.vectors_docs\n",
    "doc_tag = list(model.docvecs.doctags.keys())\n",
    "word_vectors = model.wv.vectors\n",
    "\n",
    "pickle_o = pickle_obj(); \n",
    "id_to_auth = pickle_o.load(\"../../assets/dictionaries/id_to_all_auths_2004\")\n",
    "auth_to_id = pickle_o.load(\"../../assets/dictionaries/auths_to_all_id_2004\")\n",
    "id_to_auth = pickle_o.load(\"../../assets/dictionaries/id_to_all_auths_2004\")\n",
    "auth_to_id = pickle_o.load(\"../../assets/dictionaries/auths_to_all_id_2004\")\n",
    "\n",
    "negative_test = pickle_o.load(\"../../assets/goldenstandards/test_neg_aricles\")\n",
    "negative_pos = pickle_o.load(\"../../assets/goldenstandards/test_pos_aricles\")\n",
    "\n",
    "df_auth = pd.read_csv(\"../../assets/dataframes/KT_auth_2004\")\n",
    "df_abs = pd.read_csv(\"../../assets/dataframes/all_authors_df_2004\")\n",
    "\n",
    "#negative_test = pickle_o.load(\"assets/goldenstandards/test_neg_aricles\")\n",
    "#negative_pos = pickle_o.load(\"assets/goldenstandards/test_pos_aricles\")\n",
    "\n",
    "negative_val = np.load(\"../../assets/goldenstandards/NLS_val.npy\")\n",
    "negative_test = np.load(\"../../assets/goldenstandards/NLS_test.npy\")\n",
    "postive_val = np.load(\"../../assets/goldenstandards/LS_val.npy\")\n",
    "positve_test = np.load(\"../../assets/goldenstandards/LS_test.npy\")\n",
    "\n",
    "\n",
    "tes_tag_str = list(np.array(list(negative_val) + list(negative_test)+list(postive_val)+list(positve_test)).astype(str))\n",
    "train_tag = list(set(doc_tag) - (set(tes_tag_str)))\n",
    "train_vec = model[train_tag]\n",
    "\n",
    "test_nls = list(np.asarray(negative_test).astype(str))\n",
    "val_nls = list(np.asarray(negative_val).astype(str))\n",
    "test_ls = list(np.asarray(positve_test).astype(str))\n",
    "val_ls = list(np.asarray(postive_val).astype(str))\n",
    "\n",
    "X_nls_test = model[test_nls]\n",
    "X_nls_val = model[val_nls]\n",
    "\n",
    "X_ls_test = model[test_ls]\n",
    "X_ls_val = model[val_ls]\n",
    "\n",
    "\n",
    "y_nls_test = np.ones(X_nls_test.shape[0])\n",
    "y_nls_val = np.ones(X_nls_val.shape[0])\n",
    "\n",
    "y_ls_test = np.zeros(X_ls_test.shape[0])\n",
    "y_ls_val = np.zeros(X_ls_val.shape[0])\n",
    "\n",
    "\n",
    "X_test = np.concatenate((X_nls_test, X_ls_test))\n",
    "X_val = np.concatenate((X_nls_val, X_ls_val))\n",
    "\n",
    "y_test = np.concatenate((y_nls_test, y_ls_test))\n",
    "y_val = np.concatenate((y_nls_val, y_ls_val))\n",
    "\n",
    "all_vec = np.concatenate([train_vec, word_vectors])\n",
    "tags =np.concatenate([train_tag, model.wv.index2word])\n",
    "all_vec = np.asarray(all_vec)\n",
    "tags =np.asarray(tags)\n",
    "\n",
    "col_names = pd.read_csv('../../assets/clusterModels/assignKMM_clusters1000Articles', nrows=0).columns\n",
    "types_dict = {}\n",
    "types_dict.update({col: str for col in col_names if col not in types_dict})\n",
    "df = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters1000Articles\", dtype=types_dict)\n",
    "df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "col_names = pd.read_csv('../../assets/clusterModels/assignKMM_clusters1000Articles_clusters_with_only_authors', nrows=0).columns\n",
    "types_dict = {}\n",
    "types_dict.update({col: str for col in col_names if col not in types_dict})\n",
    "df = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters1000Articles_clusters_with_only_authors\", dtype=types_dict)\n",
    "df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "\n",
    "col_names = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters300Articles_df_cluster_authors\", nrows=0).columns\n",
    "types_dict = {}\n",
    "types_dict.update({col: str for col in col_names if col not in types_dict})\n",
    "\n",
    "df_cluster_authors = pd.read_csv(\"../../assets/clusterModels/assignKMM_clusters300Articles_df_cluster_authors\", dtype=types_dict)\n",
    "df_cluster_authors.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "df_cluster_authors.columns = df_cluster_authors.columns.astype(int)\n",
    "\n",
    "postive_clusters = pickle_o.load(\"../../assets/clusterModels/KMM1000_testVal_postive_clusters\")\n",
    "negaitves_clusters = pickle_o.load(\"../../assets/clusterModels/KMM1000_testVal_negaitves_clusters\")\n",
    "\n",
    "p_list_cluster = list(set([item[0] for sublist in postive_clusters for item in sublist]))\n",
    "n_list_cluster = list(set([item[0] for sublist in negaitves_clusters for item in sublist]))\n",
    "intersect = list(set(n_list_cluster) & set(p_list_cluster))\n",
    "n_list_cluster = list(set(n_list_cluster) ^ set(intersect))\n",
    "\n",
    "not_NLS = [667, 759, 118, 237, 779, 815, 166, 716, 832, 771, 400, 200, 515, 691, 357, 19, 29, 333, 227, 277]\n",
    "extra_ls = [759, 118, 237, 779, 716, 832, 771, 400, 200, 691, 357, 333, 227, 277]\n",
    "LS = [115, 329, 423, 680, 562, 45, 178, 624, 143, 602, 784, 748, 788, 763, 645, 328, 564, 919, 444, 892, 962\n",
    "     , 24, 632, 160, 187, 936, 972, 76, 163, 533, 841, 230, 416]\n",
    "NLS = list((set(n_list_cluster) - set(not_NLS)))\n",
    "#LS = LS + extra_ls\n",
    "potential_ls = list((set(df.columns.values.astype(int)) - set(LS)) - set(NLS))\n",
    "\n",
    "#NLF_train_v, nlf_train_lab = get_labels_and_vec(df, NLS, 1)\n",
    "#LF_train_v, lf_train_lab = get_labels_and_vec(df, LS, 1)\n",
    "\n",
    "NLF_train_v, nlf_train_lab = get_train_data(df, NLS, model, 1)\n",
    "LF_train_v, lf_train_lab = get_train_data(df, LS, model, 1)\n",
    "\n",
    "X_train, y_train = concatenate_and_get_labels(LF_train_v, NLF_train_v)\n",
    "X_un_pot, L_pot = getVectors(df, model, potential_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tensorflow as tf; print(tf.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLayer:\n",
    "    \"\"\"\n",
    "    StochasticLayer Dense Layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    BN : bool\n",
    "        Batchnormalization\n",
    "    n_in : int\n",
    "        Input nodes\n",
    "    n_out: int\n",
    "        Output nodes\n",
    "    model_prob: float\n",
    "        Dropout probability\n",
    "    model_lam: float\n",
    "        regualarization term\n",
    "    is_training: bool\n",
    "        traing-phase/test-phase        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Outputlayer\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, n_in, n_out, model_prob, model_lam, is_training, BN=False, decay=0.99):\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.is_training = is_training    \n",
    "        self.model_prob = model_prob\n",
    "        self.model_lam = model_lam\n",
    "        self.model_bern = Bernoulli(probs=self.model_prob, dtype=tf.float32)\n",
    "        self.model_M = tf.Variable(tf.truncated_normal([n_in, n_out], stddev=0.01))\n",
    "        self.model_m = tf.Variable(tf.zeros([n_out]))\n",
    "        self.model_W = tf.matmul(\n",
    "            tf.diag(self.model_bern.sample((n_in, ))), self.model_M\n",
    "        )\n",
    "        #Parameters for BatchNormalization\n",
    "        self.BN = BN\n",
    "        self.scale = tf.Variable(tf.ones([n_out]))\n",
    "        self.beta = tf.Variable(tf.zeros([n_out]))\n",
    "        self.epsilon = 1e-2\n",
    "        self.mean = tf.Variable(tf.zeros([n_out]), trainable=False)\n",
    "        self.var = tf.Variable(tf.ones([n_out]), trainable=False)\n",
    "        self.decay = decay\n",
    "\n",
    "    def __call__(self, X, activation=tf.identity):\n",
    "        output = activation(tf.matmul(X, self.model_W) + self.model_m)\n",
    "        \n",
    "        if self.BN:\n",
    "            output = self.batch_norm_wrapper(output, decay = 0.5)\n",
    "                \n",
    "        if self.model_M.shape[1] == 1:\n",
    "            #output = tf.squeeze(output)\n",
    "            output = output\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def regularization(self):\n",
    "        \"\"\"regularization\"\"\"\n",
    "\n",
    "        return self.model_lam * (\n",
    "            self.model_prob * tf.reduce_sum(tf.square(self.model_M)) +\n",
    "            tf.reduce_sum(tf.square(self.model_m))\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    def batch_norm_wrapper(self, inputs, decay = 0.99):\n",
    "        \"\"\"\n",
    "        Batchnormalization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : array\n",
    "            Batch input\n",
    "        is_training : bool\n",
    "            training phase/test phase\n",
    "        decay: float\n",
    "            Decrease training of popoulation mean and variance.    \n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            Normalized batched\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        def BN_train():\n",
    "            \"\"\"\n",
    "            Batchnormalization training\n",
    " \n",
    "            Returns\n",
    "            -------\n",
    "            array\n",
    "                Normalized train batch\n",
    "            \"\"\"\n",
    "            batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "            train_mean = tf.assign(self.mean, self.mean * self.decay + batch_mean * (1 - self.decay))\n",
    "            train_var = tf.assign(self.var, self.var * self.decay + batch_var * (1 - self.decay))\n",
    "            #train_mean = tf.assign(self.mean, self.mean + batch_mean)\n",
    "            #train_var = tf.assign(self.var, self.var + batch_var)\n",
    "            \n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                return tf.nn.batch_normalization(inputs,\n",
    "                    batch_mean, batch_var, self.beta, self.scale, self.epsilon)\n",
    "            \n",
    "        def BN_test():\n",
    "            \"\"\"\n",
    "            Batchnormalization test\n",
    " \n",
    "            Returns\n",
    "            -------\n",
    "            array\n",
    "                Normalized test batched\n",
    "            \"\"\"\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                self.mean, self.var, self.beta, self.scale, self.epsilon)\n",
    "        \n",
    "        \n",
    "        return tf.cond(self.is_training, BN_train, BN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, glob\n",
    " \n",
    "def moveAllFilesinDir(srcDir, dstDir):\n",
    "    # Check if both the are directories\n",
    "    if os.path.isdir(srcDir) and os.path.isdir(dstDir) :\n",
    "        # Iterate over all the files in source directory\n",
    "        for filePath in glob.glob(srcDir + '\\*'):\n",
    "            print(filePath)\n",
    "            # Move each file to destination Directory\n",
    "            shutil.move(filePath, dstDir);\n",
    "    else:\n",
    "        print(\"srcDir & dstDir should be Directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDNNCalssifier:\n",
    "    def __init__(self, batchnorm=True, dropout_fit=0.8, dropout_sample=0.8, batch_size=256):\n",
    "        self.BN = batchnorm\n",
    "        self.dropout_fit = dropout_fit\n",
    "        self.dropout_sample = dropout_sample\n",
    "        self.bs = batch_size\n",
    "        self.load = False\n",
    "    def _build_graph(self, n_feats, n_samples, dp_prob=1.0, BN=False, decay=0.99):\n",
    "        \"\"\"\n",
    "        Building training graph\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dp_prob : float\n",
    "            Dropout prob\n",
    "        BN : bool\n",
    "            Batchnorm\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            graph outputs\n",
    "        \"\"\"\n",
    "    \n",
    "        #n_feats = X.shape[1]\n",
    "        n_hidden = 500\n",
    "        model_prob = dp_prob\n",
    "        model_lam = 1e-8\n",
    "        BN=BN\n",
    "    \n",
    "        X_input = tf.placeholder(tf.float32, [None, n_feats])\n",
    "        y_input = tf.placeholder(tf.float32, [None,1])\n",
    "    \n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "        Layer_1 = StochasticLayer(n_feats, n_hidden * 2, model_prob, model_lam, is_training ,BN=BN, decay=decay)\n",
    "        Layer_2 = StochasticLayer(n_hidden * 2, n_hidden, model_prob, model_lam, is_training,BN=BN, decay=decay)\n",
    "        Layer_3 = StochasticLayer(n_hidden, n_hidden // 2, model_prob, model_lam, is_training,BN=BN, decay=decay)\n",
    "        Layer_4 = StochasticLayer(n_hidden // 2, 1, model_prob, model_lam, is_training, BN=BN, decay=decay)\n",
    "    \n",
    "        z_1 = Layer_1(X_input, tf.nn.sigmoid)\n",
    "        z_2 = Layer_2(z_1, tf.nn.sigmoid)\n",
    "        z_3 = Layer_3(z_2, tf.nn.sigmoid)\n",
    "        y_pred = Layer_4(z_3, tf.nn.sigmoid)\n",
    "    \n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_input, logits=y_pred)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "        predicted = tf.nn.sigmoid(y_pred)\n",
    "\n",
    "        correct_pred = tf.equal(tf.round(predicted), y_input)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "        model_loss = (\n",
    "            # Negative log-likelihood.\n",
    "            loss +\n",
    "            # Regularization.\n",
    "            Layer_1.regularization +\n",
    "            Layer_2.regularization +\n",
    "            Layer_3.regularization\n",
    "                    ) / n_samples\n",
    "    \n",
    "        train_step = tf.train.AdamOptimizer(1e-3).minimize(model_loss)\n",
    "\n",
    "        \n",
    "        return (X_input, y_input), train_step, accuracy, model_loss, predicted, tf.train.Saver(), is_training\n",
    "\n",
    "    \n",
    "    def _train(self, X, y, train_step, X_input, y_input, is_training, accuracy, model_mse, saver, \n",
    "               epoch=20, verbose =True, bs=10):\n",
    "        \"\"\"\n",
    "        Train model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        iterations : int\n",
    "            Number of iterations\n",
    "        verbose : bool\n",
    "            Verbose\n",
    "        bs: int\n",
    "            batch-size\n",
    "        \"\"\"\n",
    "        with tf.Session() as sess:\n",
    "            if self.load:\n",
    "                saver.restore(sess, self.path)\n",
    "                \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epoch):\n",
    "                test_idx = np.arange(0 , len(X))\n",
    "                np.random.shuffle(test_idx)\n",
    "                for i in range(0,len(X), bs): \n",
    "                    X_batch = X[test_idx[i:(i+bs)]]\n",
    "                    y_batch = y[test_idx[i:(i+bs)]]\n",
    "            \n",
    "                    train_step.run(feed_dict={X_input: X_batch, y_input: y_batch, is_training:True})\n",
    "                if verbose:\n",
    "                    print(e)\n",
    "                    mse = sess.run(model_mse, {X_input: X_batch, y_input: y_batch, is_training:True})\n",
    "                    _accuracy = sess.run(accuracy, {X_input: X_batch, y_input: y_batch, is_training:True})\n",
    "                    print(\"Epoch {}, -log(mse): {:.2f} , acc: {:.2f}\".format(e, -np.log(mse), _accuracy))\n",
    "            \n",
    "            self.load = False\n",
    "            saved_model = saver.save(sess, 'temp/temp-bn-save')\n",
    "            sess.close()\n",
    "   \n",
    "    def fit(self,X, y, epoch):\n",
    "        self.X_train = X\n",
    "        tf.reset_default_graph()\n",
    "        (X_input, y_input), train_step, accuracy, model_mse, _, saver, is_training = self._build_graph(X.shape[1],\n",
    "                                                                                                       self.bs,\n",
    "                                                                                                       BN=self.BN, \n",
    "                                                                                                       dp_prob=self.dropout_fit)\n",
    "        self.saver = saver\n",
    "        self._train(X, y.reshape(-1,1), train_step, X_input, y_input, is_training,\n",
    "                    accuracy, model_mse, saver, epoch=epoch, verbose = True, bs=self.bs)\n",
    "    \n",
    "    def save_weights(self):\n",
    "        path = \"/home/ekvall/kth-cluster/kth-cluster/getLifeScience/BayesDNN/Feedforward_example/\"\n",
    "        if os.path.isdir(path + \"bestmodel/temp\"):\n",
    "            shutil.rmtree(path + \"bestmodel/temp\")\n",
    "            \n",
    "        shutil.move(path + \"temp\",\n",
    "                   path + \"bestmodel/\")\n",
    "        \n",
    "    def load_weights(self):\n",
    "        self.load = True\n",
    "        self.path = \"/home/ekvall/kth-cluster/kth-cluster/getLifeScience/BayesDNN/Feedforward_example/bestmodel/temp/temp-bn-save\"\n",
    "\n",
    "        \n",
    "    def _test(self, X_pred, X_input, y_input, train_step, model_pred, saver, is_training,\n",
    "              samples=1000, bs=10,BN=False):\n",
    "        \"\"\"\n",
    "        Test model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : int\n",
    "            Monte Carlo Samples\n",
    "        bs : int\n",
    "            Batchnorm samples\n",
    "        \"\"\"\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            if self.load:\n",
    "                saver.restore(sess, self.path)\n",
    "                print(\"yo\")\n",
    "            else:\n",
    "                saver.restore(sess, 'temp/temp-bn-save')\n",
    "    \n",
    "\n",
    "            Y_sample = np.zeros((samples, X_pred.shape[0]))\n",
    "            for i in range(samples):       \n",
    "                if BN:\n",
    "                    test_idx = np.arange(0 , len(X_pred))\n",
    "                    np.random.shuffle(test_idx)\n",
    "                    model_pred.eval({X_input: self.X_train[test_idx[:bs]],  is_training: True})\n",
    "                Y_sample[i] = (sess.run(model_pred, {X_input: X_pred, is_training: False})).ravel()\n",
    "            return Y_sample\n",
    "    \n",
    "    def sample_predictions(self, X_pred, mc_samples):\n",
    "        tf.reset_default_graph()\n",
    "        (X_input, y_input), train_step, accuracy, model_mse, model_pred, saver, is_training = self._build_graph(X_train.shape[1],\n",
    "                                                                                                                self.bs,\n",
    "                                                                                                                BN=self.BN, \n",
    "                                                                                                          dp_prob=self.dropout_sample)\n",
    "        Y_sample = self._test(X_pred, X_input, y_input, train_step, model_pred, saver, is_training,mc_samples,\n",
    "                              bs=self.bs, BN=False)\n",
    "        \n",
    "        return Y_sample.round()\n",
    "    \n",
    "    def predict(self, X_train, X_pred, mc_samples):\n",
    "        y_samples = self.sample_predictions(X_train, X_pred, mc_samples)\n",
    "        pred = y_samples.sum(axis=0)\n",
    "        pred[pred <= mc_samples //2] = 0\n",
    "        pred[pred > mc_samples //2] = 1\n",
    "        return pred\n",
    "    \n",
    "    def probability(self, X_train, X_pred, mc_samples):\n",
    "        n_1 = self.sample_predictions(X_train, X_pred, mc_samples).sum(axis=0)\n",
    "        N = n_1.copy()\n",
    "        n_0 = mc_samples - n_1\n",
    "        N[n_0 >= n_1] = n_0[n_0 >= n_1]\n",
    "        P = N / mc_samples * 100\n",
    "        return P\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = BayesDNNCalssifier(batchnorm=True, dropout_fit=0.7, dropout_sample=0.6, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0, -log(mse): 6.56 , acc: 0.92\n",
      "1\n",
      "Epoch 1, -log(mse): 6.43 , acc: 0.90\n",
      "2\n",
      "Epoch 2, -log(mse): 6.44 , acc: 0.88\n",
      "3\n",
      "Epoch 3, -log(mse): 6.61 , acc: 0.88\n",
      "4\n",
      "Epoch 4, -log(mse): 6.60 , acc: 0.90\n",
      "5\n",
      "Epoch 5, -log(mse): 6.73 , acc: 0.91\n",
      "6\n",
      "Epoch 6, -log(mse): 6.74 , acc: 0.90\n",
      "7\n",
      "Epoch 7, -log(mse): 6.77 , acc: 0.89\n",
      "8\n",
      "Epoch 8, -log(mse): 6.73 , acc: 0.89\n",
      "9\n",
      "Epoch 9, -log(mse): 6.94 , acc: 0.92\n",
      "10\n",
      "Epoch 10, -log(mse): 6.96 , acc: 0.93\n",
      "11\n",
      "Epoch 11, -log(mse): 6.65 , acc: 0.89\n",
      "12\n",
      "Epoch 12, -log(mse): 6.98 , acc: 0.93\n",
      "13\n",
      "Epoch 13, -log(mse): 7.08 , acc: 0.90\n",
      "14\n",
      "Epoch 14, -log(mse): 7.07 , acc: 0.92\n",
      "15\n",
      "Epoch 15, -log(mse): 7.04 , acc: 0.96\n",
      "16\n",
      "Epoch 16, -log(mse): 7.17 , acc: 0.89\n",
      "17\n",
      "Epoch 17, -log(mse): 6.91 , acc: 0.89\n",
      "18\n",
      "Epoch 18, -log(mse): 6.99 , acc: 0.92\n",
      "19\n",
      "Epoch 19, -log(mse): 7.08 , acc: 0.93\n",
      "20\n",
      "Epoch 20, -log(mse): 6.94 , acc: 0.92\n",
      "21\n",
      "Epoch 21, -log(mse): 7.18 , acc: 0.91\n",
      "22\n",
      "Epoch 22, -log(mse): 7.13 , acc: 0.93\n",
      "23\n",
      "Epoch 23, -log(mse): 7.43 , acc: 0.93\n",
      "24\n",
      "Epoch 24, -log(mse): 7.38 , acc: 0.94\n",
      "25\n",
      "Epoch 25, -log(mse): 7.10 , acc: 0.95\n",
      "26\n",
      "Epoch 26, -log(mse): 7.69 , acc: 0.94\n",
      "27\n",
      "Epoch 27, -log(mse): 7.51 , acc: 0.95\n",
      "28\n",
      "Epoch 28, -log(mse): 7.25 , acc: 0.96\n",
      "29\n",
      "Epoch 29, -log(mse): 7.36 , acc: 0.93\n",
      "30\n",
      "Epoch 30, -log(mse): 7.24 , acc: 0.95\n",
      "31\n",
      "Epoch 31, -log(mse): 7.38 , acc: 0.96\n",
      "32\n",
      "Epoch 32, -log(mse): 7.33 , acc: 0.96\n",
      "33\n",
      "Epoch 33, -log(mse): 7.33 , acc: 0.96\n",
      "34\n",
      "Epoch 34, -log(mse): 7.36 , acc: 0.94\n",
      "35\n",
      "Epoch 35, -log(mse): 7.86 , acc: 0.97\n",
      "36\n",
      "Epoch 36, -log(mse): 7.31 , acc: 0.96\n",
      "37\n",
      "Epoch 37, -log(mse): 7.26 , acc: 0.95\n",
      "38\n",
      "Epoch 38, -log(mse): 7.75 , acc: 0.97\n",
      "39\n",
      "Epoch 39, -log(mse): 7.56 , acc: 0.97\n",
      "40\n",
      "Epoch 40, -log(mse): 7.57 , acc: 0.98\n",
      "41\n",
      "Epoch 41, -log(mse): 7.33 , acc: 0.95\n",
      "42\n",
      "Epoch 42, -log(mse): 7.92 , acc: 0.95\n",
      "43\n",
      "Epoch 43, -log(mse): 7.91 , acc: 0.98\n",
      "44\n",
      "Epoch 44, -log(mse): 8.52 , acc: 0.98\n",
      "45\n",
      "Epoch 45, -log(mse): 8.25 , acc: 0.96\n",
      "46\n",
      "Epoch 46, -log(mse): 7.63 , acc: 0.97\n",
      "47\n",
      "Epoch 47, -log(mse): 7.30 , acc: 0.96\n",
      "48\n",
      "Epoch 48, -log(mse): 8.12 , acc: 0.95\n",
      "49\n",
      "Epoch 49, -log(mse): 7.95 , acc: 0.98\n",
      "50\n",
      "Epoch 50, -log(mse): 8.05 , acc: 0.95\n",
      "51\n",
      "Epoch 51, -log(mse): 7.48 , acc: 0.98\n",
      "52\n",
      "Epoch 52, -log(mse): 8.34 , acc: 0.99\n",
      "53\n",
      "Epoch 53, -log(mse): 8.49 , acc: 0.98\n",
      "54\n",
      "Epoch 54, -log(mse): 7.55 , acc: 0.97\n",
      "55\n",
      "Epoch 55, -log(mse): 7.86 , acc: 0.98\n",
      "56\n",
      "Epoch 56, -log(mse): 7.52 , acc: 0.96\n",
      "57\n",
      "Epoch 57, -log(mse): 8.18 , acc: 0.98\n",
      "58\n",
      "Epoch 58, -log(mse): 8.14 , acc: 0.98\n",
      "59\n",
      "Epoch 59, -log(mse): 9.30 , acc: 0.98\n",
      "60\n",
      "Epoch 60, -log(mse): 8.14 , acc: 0.98\n",
      "61\n",
      "Epoch 61, -log(mse): 7.83 , acc: 0.98\n",
      "62\n",
      "Epoch 62, -log(mse): 7.82 , acc: 0.95\n",
      "63\n",
      "Epoch 63, -log(mse): 8.52 , acc: 0.98\n",
      "64\n",
      "Epoch 64, -log(mse): 8.66 , acc: 0.98\n",
      "65\n",
      "Epoch 65, -log(mse): 7.98 , acc: 0.94\n",
      "66\n",
      "Epoch 66, -log(mse): 8.13 , acc: 0.99\n",
      "67\n",
      "Epoch 67, -log(mse): 7.96 , acc: 0.95\n",
      "68\n",
      "Epoch 68, -log(mse): 9.00 , acc: 0.99\n",
      "69\n",
      "Epoch 69, -log(mse): 8.04 , acc: 0.97\n",
      "70\n",
      "Epoch 70, -log(mse): 7.99 , acc: 0.97\n",
      "71\n",
      "Epoch 71, -log(mse): 8.17 , acc: 0.96\n",
      "72\n",
      "Epoch 72, -log(mse): 8.47 , acc: 0.98\n",
      "73\n",
      "Epoch 73, -log(mse): 8.65 , acc: 0.98\n",
      "74\n",
      "Epoch 74, -log(mse): 8.12 , acc: 0.99\n",
      "75\n",
      "Epoch 75, -log(mse): 8.51 , acc: 1.00\n",
      "76\n",
      "Epoch 76, -log(mse): 9.39 , acc: 0.98\n",
      "77\n",
      "Epoch 77, -log(mse): 7.85 , acc: 0.98\n",
      "78\n",
      "Epoch 78, -log(mse): 8.28 , acc: 0.95\n",
      "79\n",
      "Epoch 79, -log(mse): 8.57 , acc: 0.99\n",
      "80\n",
      "Epoch 80, -log(mse): 8.96 , acc: 0.97\n",
      "81\n",
      "Epoch 81, -log(mse): 8.41 , acc: 0.98\n",
      "82\n",
      "Epoch 82, -log(mse): 8.02 , acc: 0.98\n",
      "83\n",
      "Epoch 83, -log(mse): 8.92 , acc: 1.00\n",
      "84\n",
      "Epoch 84, -log(mse): 9.24 , acc: 0.98\n",
      "85\n",
      "Epoch 85, -log(mse): 7.89 , acc: 0.98\n",
      "86\n",
      "Epoch 86, -log(mse): 7.65 , acc: 1.00\n",
      "87\n",
      "Epoch 87, -log(mse): 9.01 , acc: 0.98\n",
      "88\n",
      "Epoch 88, -log(mse): 8.67 , acc: 0.98\n",
      "89\n",
      "Epoch 89, -log(mse): 9.00 , acc: 1.00\n",
      "90\n",
      "Epoch 90, -log(mse): 8.82 , acc: 0.99\n",
      "91\n",
      "Epoch 91, -log(mse): 9.06 , acc: 0.99\n",
      "92\n",
      "Epoch 92, -log(mse): 8.27 , acc: 0.98\n",
      "93\n",
      "Epoch 93, -log(mse): 8.33 , acc: 0.98\n",
      "94\n",
      "Epoch 94, -log(mse): 10.19 , acc: 0.98\n",
      "95\n",
      "Epoch 95, -log(mse): 8.86 , acc: 0.98\n",
      "96\n",
      "Epoch 96, -log(mse): 8.37 , acc: 0.98\n",
      "97\n",
      "Epoch 97, -log(mse): 9.45 , acc: 0.98\n",
      "98\n",
      "Epoch 98, -log(mse): 8.09 , acc: 0.99\n",
      "99\n",
      "Epoch 99, -log(mse): 8.09 , acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "dnn.fit(X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/temp-bn-save\n"
     ]
    }
   ],
   "source": [
    "pred_test = dnn.predict(X_train, X_test, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ekvall/kth-cluster/kth-cluster/getLifeScience/BayesDNN/Feedforward_example/bestmodel/temp/temp-bn-save\n",
      "yo\n"
     ]
    }
   ],
   "source": [
    "pred_val = dnn.predict(X_train, X_val, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘bestmodel’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestmodel   temp-bn-save\t\t      temp-bn-save.index\r\n",
      "checkpoint  temp-bn-save.data-00000-of-00001  temp-bn-save.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"bestmodel/temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9373040752351097"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9341085271317829"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ix = pred == y_val\n",
    "false_ix = np.invert(correct_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pre[false_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[false_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred_pre,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sample.astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename=\"./assets/point_estimate.jpg\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian - Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: \n",
    "\n",
    "Article: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\n",
    "\n",
    "Authors: Yarin Gal, Zoubin Ghahramani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, _, saver, is_training = build_graph(BN=False, dp_prob=0.9)\n",
    "train(verbose = False, bs=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, model_pred, saver, is_training = build_graph(BN=False, dp_prob=0.9)\n",
    "Y_sample_DP = test(mc_samples, bs=n_samples)\n",
    "plot(Y_sample_DP, X_pred, mc_samples,color_line=\"b-\", path=\"./assets/non_DO_BN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian - BatchNormalization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit:\n",
    "\n",
    "Aricle: Bayesian Uncertainty Estimation for Batch Normalized Deep Networks\n",
    "\n",
    "Authors: Mattias Teye, Hossein Azizpour, Kevin Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, _, saver, is_training = build_graph(BN=True, dp_prob=1.0, decay=0.6)\n",
    "train(verbose = False , bs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, model_pred, saver, is_training = build_graph(BN=True, dp_prob=1.0, decay=0.6)\n",
    "Y_sample_BN = test(mc_samples, bs=15,BN=True)\n",
    "plot(Y_sample_BN, X_pred,mc_samples, color_line=\"g-\",path=\"./assets/BN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        for i in range(mc_samples):\n",
    "            plt.plot(X_pred[:, 0], Y_sample_BN[i], \"b-\", alpha=1. / 200)\n",
    "        for i in range(mc_samples):\n",
    "            plt.plot(X_pred[:, 0], Y_sample_DP[i], \"r-\", alpha=1. / 200)\n",
    "        plt.plot(X[:, 0], y, \"g.\")\n",
    "        \n",
    "        \n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        #if path:\n",
    "        #    plt.savefig(path, format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian - BatchNormalization and Drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, _, saver, is_training = build_graph(BN=True, dp_prob=0.85, decay=0.5)\n",
    "train(verbose = False, bs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "(X_input, y_input), train_step, model_mse, model_pred, saver, is_training = build_graph(BN=True, dp_prob=0.85, decay=0.5)\n",
    "Y_sample = test(mc_samples, bs=20,BN=True)\n",
    "plot(Y_sample, X_pred,mc_samples, color_line=\"b-\",path=\"./assets/BN_DO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
